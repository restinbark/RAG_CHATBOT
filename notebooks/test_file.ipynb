{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5718736a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'ArceeForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'Dots1ForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconH1ForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'Gemma3nForConditionalGeneration', 'Gemma3nForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MiniMaxForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'SmolLM3ForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Answer:\n",
      " \n",
      "You are a financial analyst assistant for CrediTrust. Your task is to answer questions about customer complaints.\n",
      "\n",
      "Use the following retrieved complaint excerpts to formulate your answer. If the context doesn't contain the answer, say so.\n",
      "\n",
      "Context:\n",
      "would make it right for their customer that always pay more than due and early\n",
      "\n",
      "and then abruptly closing consumers accounts who have been great customers paying their bills on time this also hurts the stores because they lose tons of business from good paying customers who pay their bills\n",
      "\n",
      "in purchases in the next statement this is a huge bookkeeping mistake because its neither purchase or return a bellyflop disputes about no customer service in span of two years or more slow response no response or inability to problem solve as problem arises this is why same disputes are still\n",
      "\n",
      "to allow payments to be made online after xxxx days they clearly want to make things difficult for their customers and increase the likelihood of late fee revenue by forcing payments to be made through slow and inconvenient methods for no reasons other than truist s animosity and greed xxxx there\n",
      "\n",
      "care about the consumer one bit they are happy collecting late charges without providing the consumer due process to rectify an oversight on my part by not paying the balance in full instead they simply charge it off and refuse to deal with the consumer past that point if you look at the history on\n",
      "\n",
      "Question:\n",
      "Why are customers unhappy with Buy Now, Pay Later?\n",
      "\n",
      "Answer:\n",
      "\n",
      "ðŸ“š Sources:\n",
      "1. would make it right for their customer that always pay more than due and early...\n",
      "2. and then abruptly closing consumers accounts who have been great customers paying their bills on time this also hurts th...\n",
      "3. in purchases in the next statement this is a huge bookkeeping mistake because its neither purchase or return a bellyflop...\n",
      "4. to allow payments to be made online after xxxx days they clearly want to make things difficult for their customers and i...\n",
      "5. care about the consumer one bit they are happy collecting late charges without providing the consumer due process to rec...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.rag_pipeline import ask_question\n",
    "\n",
    "# Question to ask the chatbot\n",
    "question = \"Why are customers unhappy with Buy Now, Pay Later?\"\n",
    "\n",
    "# Set correct path from inside notebooks/\n",
    "index_path = \"../vector_store/faiss_index\"\n",
    "\n",
    "# Ask your chatbot\n",
    "answer, sources = ask_question(question, index_path=index_path)\n",
    "\n",
    "# Display answer and retrieved chunks\n",
    "print(\"ðŸ”Ž Answer:\\n\", answer)\n",
    "print(\"\\nðŸ“š Sources:\")\n",
    "for i, s in enumerate(sources, 1):\n",
    "    print(f\"{i}. {s[:120]}...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
